#!/usr/bin/env python3
"""
æ–°é—»çƒ­ç‚¹æ¦œåŠŸèƒ½
è¯†åˆ«ç›¸ä¼¼æ–°é—»ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„ï¼Œç»Ÿè®¡æŠ¥é“æ•°é‡
"""
import pandas as pd
from collections import defaultdict
import re
from difflib import SequenceMatcher
from typing import List, Tuple, Dict

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ"""
    if not text:
        return ""
    # ç§»é™¤æ ‡ç‚¹ã€è½¬æ¢ä¸ºå°å†™
    text = re.sub(r'[^\w\s]', '', str(text).lower())
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    return text

def similarity_score(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    if not text1 or not text2:
        return 0.0
    clean1 = clean_text(text1)
    clean2 = clean_text(text2)
    if not clean1 or not clean2:
        return 0.0
    return SequenceMatcher(None, clean1, clean2).ratio()

def are_similar_articles(row1: pd.Series, row2: pd.Series, threshold: float = 0.6) -> bool:
    """
    åˆ¤æ–­ä¸¤ç¯‡æ–‡ç« æ˜¯å¦ç›¸ä¼¼ï¼ˆåŒä¸€äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼‰
    
    ä½¿ç”¨æ ‡é¢˜å’Œ Nut Graph çš„ç›¸ä¼¼åº¦
    """
    # æå–æ ‡é¢˜å’Œå†…å®¹
    headline1 = str(row1.get('Headline', ''))
    headline2 = str(row2.get('Headline', ''))
    nut1 = str(row1.get('Nut Graph', ''))
    nut2 = str(row2.get('Nut Graph', ''))
    
    # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
    headline_sim = similarity_score(headline1, headline2)
    
    # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦ä¸å¤Ÿï¼Œå†æ£€æŸ¥å†…å®¹ï¼‰
    if headline_sim < threshold:
        # ç»„åˆæ ‡é¢˜å’Œå†…å®¹
        text1 = f"{headline1} {nut1}"
        text2 = f"{headline2} {nut2}"
        combined_sim = similarity_score(text1, text2)
        return combined_sim >= threshold
    
    return headline_sim >= threshold

def group_similar_news(df: pd.DataFrame, similarity_threshold: float = 0.6) -> pd.DataFrame:
    """
    å°†ç›¸ä¼¼æ–°é—»åˆ†ç»„
    
    Returns:
        DataFrame with 'GroupID' column indicating which articles are similar
    """
    if df.empty:
        return df
    
    df = df.copy()
    df['GroupID'] = -1
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿è¿½è¸ª
    df = df.reset_index(drop=True)
    
    group_id = 0
    processed = set()
    
    for i in range(len(df)):
        if i in processed:
            continue
        
        # åˆ›å»ºæ–°ç»„
        df.loc[i, 'GroupID'] = group_id
        processed.add(i)
        
        # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« 
        for j in range(i + 1, len(df)):
            if j in processed:
                continue
            
            if are_similar_articles(df.iloc[i], df.iloc[j], similarity_threshold):
                df.loc[j, 'GroupID'] = group_id
                processed.add(j)
        
        group_id += 1
    
    return df

def generate_trending_rank(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame:
    """
    ç”Ÿæˆçƒ­ç‚¹æ¦œ
    
    Args:
        df: åŒ…å« GroupID åˆ—çš„ DataFrame
        top_n: æ¯ä¸ªç±»åˆ«æ˜¾ç¤º top N æ¡æ–°é—»
    
    Returns:
        DataFrame with trending information
    """
    if df.empty or 'GroupID' not in df.columns:
        return pd.DataFrame()
    
    # æŒ‰ GroupID å’Œ Category åˆ†ç»„
    trending_news = []
    
    for category in df['Category'].unique():
        category_df = df[df['Category'] == category]
        
        # æŒ‰ GroupID åˆ†ç»„
        for group_id in category_df['GroupID'].unique():
            group_df = category_df[category_df['GroupID'] == group_id]
            
            if len(group_df) < 2:  # è‡³å°‘éœ€è¦2å®¶åª’ä½“æŠ¥é“
                continue
            
            # ç»Ÿè®¡ä¿¡æ¯
            source_count = len(group_df)
            outlets = group_df['Outlet'].unique().tolist()
            
            # é€‰æ‹©ä»£è¡¨æ€§çš„æ ‡é¢˜ï¼ˆæœ€é•¿çš„æˆ–ç¬¬ä¸€ä¸ªï¼‰
            representative = group_df.iloc[0]
            
            # è·å–æ‰€æœ‰ URL
            urls = group_df['URL'].dropna().unique().tolist()
            
            # è·å–æ—¥æœŸèŒƒå›´
            dates = pd.to_datetime(group_df['Date'], errors='coerce').dropna()
            date_range = None
            if len(dates) > 0:
                date_range = dates.min().strftime('%Y-%m-%d')
            
            trending_news.append({
                'Category': category,
                'GroupID': group_id,
                'Headline': representative['Headline'],
                'SourceCount': source_count,
                'Outlets': ', '.join(outlets),
                'OutletList': outlets,
                'URLs': urls,
                'Date': date_range,
                'URL': urls[0] if urls else None
            })
    
    if not trending_news:
        return pd.DataFrame()
    
    trending_df = pd.DataFrame(trending_news)
    
    # æŒ‰ç±»åˆ«å’ŒæŠ¥é“æ•°é‡æ’åº
    trending_df = trending_df.sort_values(
        ['Category', 'SourceCount'], 
        ascending=[True, False]
    )
    
    # æ¯ä¸ªç±»åˆ«å– top N
    top_trending = []
    for category in trending_df['Category'].unique():
        category_trending = trending_df[trending_df['Category'] == category].head(top_n)
        top_trending.append(category_trending)
    
    if top_trending:
        return pd.concat(top_trending, ignore_index=True)
    else:
        return pd.DataFrame()

def format_trending_display(trending_df: pd.DataFrame) -> str:
    """æ ¼å¼åŒ–çƒ­ç‚¹æ¦œæ˜¾ç¤º"""
    if trending_df.empty:
        return "æš‚æ— çƒ­ç‚¹æ–°é—»"
    
    output = []
    current_category = None
    
    for _, row in trending_df.iterrows():
        if row['Category'] != current_category:
            current_category = row['Category']
            output.append(f"\n### ğŸ“Š {current_category}\n")
        
        output.append(f"**ğŸ”¥ {row['SourceCount']} å®¶åª’ä½“æŠ¥é“**: {row['Headline'][:100]}...")
        output.append(f"   - åª’ä½“: {row['Outlets']}")
        if row.get('Date'):
            output.append(f"   - æ—¥æœŸ: {row['Date']}")
        output.append("")
    
    return "\n".join(output)

