#!/usr/bin/env python3
"""
æ–°é—»çƒ­ç‚¹æ¦œåŠŸèƒ½
è¯†åˆ«ç›¸ä¼¼æ–°é—»ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„ï¼Œç»Ÿè®¡æŠ¥é“æ•°é‡
"""
import pandas as pd
from collections import defaultdict
import re
from difflib import SequenceMatcher
from typing import List, Tuple, Dict, Optional

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ"""
    if not text:
        return ""
    # ç§»é™¤æ ‡ç‚¹ã€è½¬æ¢ä¸ºå°å†™
    text = re.sub(r'[^\w\s]', '', str(text).lower())
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    return text

def similarity_score(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    if not text1 or not text2:
        return 0.0
    clean1 = clean_text(text1)
    clean2 = clean_text(text2)
    if not clean1 or not clean2:
        return 0.0
    return SequenceMatcher(None, clean1, clean2).ratio()

def are_similar_articles_api(
    headline1: str,
    nut1: str,
    headline2: str,
    nut2: str,
    outlet1: str = "",
    outlet2: str = "",
    date1: str = "",
    date2: str = "",
) -> Optional[bool]:
    """
    ä½¿ç”¨ OpenAI API åˆ¤æ–­ä¸¤ç¯‡æ–‡ç« æ˜¯å¦ç›¸ä¼¼ï¼ˆåŒä¸€äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼‰
    
    Returns:
        True: ç›¸ä¼¼ï¼ˆåŒä¸€äº‹ä»¶ï¼‰
        False: ä¸ç›¸ä¼¼ï¼ˆä¸åŒäº‹ä»¶ï¼‰
        None: API ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦å›é€€
    """
    try:
        import os
        from openai import OpenAI
        
        # ä»ç¯å¢ƒå˜é‡æˆ– Streamlit secrets è·å– API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            try:
                import streamlit as st
                if hasattr(st, "secrets") and "api" in st.secrets:
                    api_key = st.secrets.get("api", {}).get("openai_api_key")
            except:
                pass
        
        if not api_key:
            return None
        
        # æ£€æŸ¥é¢„ç®—ï¼ˆä½¿ç”¨ api_classifier çš„é¢„ç®—æ§åˆ¶ï¼‰
        try:
            from api_classifier import _budget_allows_call, _record_call
            if not _budget_allows_call():
                return None
        except ImportError:
            pass
        
        client = OpenAI(api_key=api_key)
        
        # æ„å»ºæ”¹è¿›çš„ promptï¼ˆæ›´å¥½åœ°è¯†åˆ«åœ°ç¼˜æ”¿æ²»ç›¸å…³æ–°é—»ï¼‰
        prompt = f"""You are a news analysis assistant. Determine if these two news articles are about the same event/story, even if reported by different outlets.

**IMPORTANT**: Articles about the same geopolitical event should be grouped together, even if they focus on different aspects or use different wording.

Examples of articles that should be grouped:
- "China-Japan tensions over Taiwan" and "Japan-China relations strained by Taiwan issue" â†’ YES (same event, different wording)
- "China rolls out red carpet for Thailand's king" and "Thailand's king visits China" â†’ YES (same event)
- "US-China trade talks resume" and "Bilateral trade negotiations between US and China" â†’ YES (same event)

Article 1:
Headline: {headline1}
Summary: {nut1}
Outlet: {outlet1}
Date: {date1}

Article 2:
Headline: {headline2}
Summary: {nut2}
Outlet: {outlet2}
Date: {date2}

Are these two articles about the same event/story? Consider:
- Same core event/fact (even if reported from different angles)
- Same time period (within a few days)
- Same key actors/entities (e.g., China, Japan, Taiwan, US, etc.)
- Same geopolitical context (e.g., Taiwan tensions, trade disputes, diplomatic visits)
- Different outlets may report the same story differently (different wording, different focus)
- Articles about the same topic but different aspects should still be grouped (e.g., "China-Japan tensions" and "Japan-China relations strained")

Respond with ONLY "yes" or "no", nothing else."""
        
        # ä»ç¯å¢ƒå˜é‡æˆ– Streamlit secrets è·å–æ¨¡å‹
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        try:
            import streamlit as st
            if hasattr(st, "secrets") and "api" in st.secrets:
                model = st.secrets.get("api", {}).get("openai_model", model)
        except:
            pass
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a news analysis assistant. Respond with only 'yes' or 'no'."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # Low temperature for consistent results
            max_tokens=10
        )
        
        result = response.choices[0].message.content.strip().lower()
        
        # è®°å½• API è°ƒç”¨
        try:
            from api_classifier import _record_call
            _record_call()
        except:
            pass
        
        # æ·»åŠ å»¶è¿Ÿé¿å… rate limit
        # Trending news å¯èƒ½æœ‰å¾ˆå¤šæ¬¡è°ƒç”¨ï¼Œéœ€è¦æ›´ä¿å®ˆçš„å»¶è¿Ÿ
        import time
        time.sleep(0.2)  # 200ms delay between requests (more conservative for trending)
        
        if "yes" in result:
            return True
        elif "no" in result:
            return False
        else:
            # å¦‚æœè¿”å›æ ¼å¼ä¸å¯¹ï¼Œè¿”å› None ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦å›é€€
            return None
            
    except ImportError:
        return None
    except Exception as e:
        # API è°ƒç”¨å¤±è´¥ï¼Œè¿”å› None ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦å›é€€
        print(f"âš ï¸ API similarity check failed: {e}")
        return None

def are_similar_articles(row1: pd.Series, row2: pd.Series, threshold: float = 0.7, use_api: bool = True, api_threshold: float = 0.4) -> bool:
    """
    åˆ¤æ–­ä¸¤ç¯‡æ–‡ç« æ˜¯å¦ç›¸ä¼¼ï¼ˆåŒä¸€äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼‰
    
    ä½¿ç”¨æ··åˆç­–ç•¥ï¼šå…ˆç”¨æ–‡å­—ç›¸ä¼¼åº¦å¿«é€Ÿç­›é€‰ï¼Œåªæœ‰"å¯èƒ½ç›¸ä¼¼"çš„æ‰ç”¨ API ç¡®è®¤
    
    Args:
        row1, row2: æ–‡ç« æ•°æ®
        threshold: æ–‡å­—ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé«˜äºæ­¤å€¼ç›´æ¥åˆ¤æ–­ä¸ºç›¸ä¼¼ï¼‰
        use_api: æ˜¯å¦ä½¿ç”¨ API åˆ¤æ–­
        api_threshold: API ç­›é€‰é˜ˆå€¼ï¼ˆåªæœ‰æ–‡å­—ç›¸ä¼¼åº¦ > æ­¤å€¼çš„æ‰ç”¨ API ç¡®è®¤ï¼‰
    """
    # æå–æ ‡é¢˜å’Œå†…å®¹
    headline1 = str(row1.get('Headline', ''))
    headline2 = str(row2.get('Headline', ''))
    nut1 = str(row1.get('Nut Graph', ''))
    nut2 = str(row2.get('Nut Graph', ''))
    outlet1 = str(row1.get('Outlet', ''))
    outlet2 = str(row2.get('Outlet', ''))
    date1 = str(row1.get('Date', ''))
    date2 = str(row2.get('Date', ''))
    
    # ============ æ··åˆç­–ç•¥ ============
    # æ­¥éª¤ 1: å…ˆç”¨æ–‡å­—ç›¸ä¼¼åº¦å¿«é€Ÿç­›é€‰
    headline_sim = similarity_score(headline1, headline2)
    
    # å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦å¾ˆä½ï¼Œç›´æ¥åˆ¤æ–­ä¸ºä¸ç›¸ä¼¼ï¼ˆå¿«é€Ÿæ’é™¤ï¼‰
    if headline_sim < 0.3:
        return False
    
    # å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œç›´æ¥åˆ¤æ–­ä¸ºç›¸ä¼¼ï¼ˆæ— éœ€ APIï¼‰
    if headline_sim >= threshold:
        return True
    
    # è®¡ç®—ç»„åˆæ–‡å­—ç›¸ä¼¼åº¦
    text1 = f"{headline1} {nut1}"
    text2 = f"{headline2} {nut2}"
    combined_sim = similarity_score(text1, text2)
    
    # å¦‚æœç»„åˆç›¸ä¼¼åº¦å¾ˆä½ï¼Œç›´æ¥åˆ¤æ–­ä¸ºä¸ç›¸ä¼¼
    if combined_sim < 0.3:
        return False
    
    # å¦‚æœç»„åˆç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œç›´æ¥åˆ¤æ–­ä¸ºç›¸ä¼¼ï¼ˆæ— éœ€ APIï¼‰
    if combined_sim >= threshold:
        return True
    
    # æ­¥éª¤ 2: ä¸­é—´åœ°å¸¦ï¼ˆ0.4 - 0.7ï¼‰- ç”¨ API ç¡®è®¤
    # åªå¯¹"è¾ƒå¯èƒ½ç›¸ä¼¼"çš„æ–‡ç« æ‰ç”¨ APIï¼Œè¿›ä¸€æ­¥å‡å°‘ API è°ƒç”¨
    # é™ä½ api_threshold ä» 0.45 åˆ° 0.40ï¼Œè®©æ›´å¤šè¾¹ç•Œæƒ…å†µä½¿ç”¨ API åˆ¤æ–­ï¼ˆæé«˜åœ°ç¼˜æ”¿æ²»æ–°é—»è¯†åˆ«ï¼‰
    if use_api and combined_sim >= max(api_threshold, 0.40):
        api_result = are_similar_articles_api(
            headline1, nut1, headline2, nut2,
            outlet1, outlet2, date1, date2
        )
        if api_result is not None:
            return api_result
    
    # æ­¥éª¤ 3: å¦‚æœ API ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨æ–‡å­—ç›¸ä¼¼åº¦
    return combined_sim >= threshold

def group_similar_news(df: pd.DataFrame, similarity_threshold: float = 0.7, min_group_size: int = 2, use_api: bool = True) -> pd.DataFrame:
    """
    å°†ç›¸ä¼¼æ–°é—»åˆ†ç»„ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼Œç„¶ååœ¨æ¯ä¸ªç±»åˆ«å†…è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥ï¼‰
    
    Returns:
        DataFrame with 'GroupID' column indicating which articles are similar
    """
    if df.empty:
        return df
    
    df = df.copy()
    df['GroupID'] = -1
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿è¿½è¸ª
    df = df.reset_index(drop=True)
    
    # å…ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼Œç„¶ååœ¨æ¯ä¸ªç±»åˆ«å†…è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥
    # è¿™æ ·å¯ä»¥ç¡®ä¿åŒä¸€ç±»åˆ«å†…çš„ç›¸ä¼¼æ–‡ç« è¢«æ­£ç¡®åˆ†ç»„
    if 'Category' not in df.columns:
        # å¦‚æœæ²¡æœ‰ç±»åˆ«åˆ—ï¼Œä½¿ç”¨å…¨å±€åˆ†ç»„
        category_groups = [('All', df)]
    else:
        # æŒ‰ç±»åˆ«åˆ†ç»„
        category_groups = [(cat, df[df['Category'] == cat]) for cat in df['Category'].unique()]
    
    group_id = 0
    
    # å¯¹æ¯ä¸ªç±»åˆ«åˆ†åˆ«è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥
    for category, category_df in category_groups:
        if category_df.empty:
            continue
        
        # ä¿å­˜åŸå§‹ç´¢å¼•
        original_indices = category_df.index.tolist()
        processed = set()
        
        # åœ¨è¯¥ç±»åˆ«å†…è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥
        for i in range(len(category_df)):
            original_i = original_indices[i]
            if original_i in processed:
                continue
            
            # åˆ›å»ºæ–°ç»„
            df.loc[original_i, 'GroupID'] = group_id
            processed.add(original_i)
            
            # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« ï¼ˆåªåœ¨è¯¥ç±»åˆ«å†…ï¼‰
            for j in range(i + 1, len(category_df)):
                original_j = original_indices[j]
                if original_j in processed:
                    continue
                
                if are_similar_articles(
                    category_df.iloc[i],
                    category_df.iloc[j],
                    threshold=similarity_threshold,
                    use_api=use_api
                ):
                    df.loc[original_j, 'GroupID'] = group_id
                    processed.add(original_j)
            
            group_id += 1
    
    return df

def generate_trending_rank(df: pd.DataFrame, top_n: int = 3, min_sources: int = 3) -> pd.DataFrame:
    """
    ç”Ÿæˆçƒ­ç‚¹æ¦œï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„æ˜¾ç¤ºï¼‰
    
    Args:
        df: åŒ…å« GroupID åˆ—çš„ DataFrame
        top_n: æ¯ä¸ªç±»åˆ«æ˜¾ç¤º top N æ¡æ–°é—»
        min_sources: æœ€å°‘éœ€è¦å¤šå°‘å®¶åª’ä½“æŠ¥é“æ‰ç®—çƒ­ç‚¹ï¼ˆé»˜è®¤ 3ï¼‰
    
    Returns:
        DataFrame with trending information
    """
    if df.empty or 'GroupID' not in df.columns:
        return pd.DataFrame()
    
    # æŒ‰ç±»åˆ«å’Œ GroupID åˆ†ç»„
    trending_news = []
    
    for category in df['Category'].unique():
        category_df = df[df['Category'] == category]
        
        # æŒ‰ GroupID åˆ†ç»„
        for group_id in category_df['GroupID'].unique():
            if group_id == -1:  # è·³è¿‡æœªåˆ†ç»„çš„æ–‡ç« 
                continue
            
            group_df = category_df[category_df['GroupID'] == group_id]
            
            # è‡³å°‘éœ€è¦ min_sources å®¶åª’ä½“æŠ¥é“
            if len(group_df) < min_sources:
                continue
            
            # ç»Ÿè®¡ä¿¡æ¯
            source_count = len(group_df)
            outlets = group_df['Outlet'].unique().tolist()
            
            # é€‰æ‹©ä»£è¡¨æ€§çš„æ ‡é¢˜ï¼ˆæœ€é•¿çš„æˆ–ç¬¬ä¸€ä¸ªï¼‰
            representative = group_df.iloc[0]
            
            # è·å–æ‰€æœ‰ URL
            urls = group_df['URL'].dropna().unique().tolist()
            
            # è·å–æ—¥æœŸèŒƒå›´
            dates = pd.to_datetime(group_df['Date'], errors='coerce').dropna()
            date_range = None
            if len(dates) > 0:
                date_range = dates.min().strftime('%Y-%m-%d')
            
            trending_news.append({
                'Category': category,
                'GroupID': group_id,
                'Headline': representative['Headline'],
                'SourceCount': source_count,
                'Outlets': ', '.join(outlets),
                'OutletList': outlets,
                'URLs': urls,
                'Date': date_range,
                'URL': urls[0] if urls else None
            })
    
    if not trending_news:
        return pd.DataFrame()
    
    trending_df = pd.DataFrame(trending_news)
    
    # æŒ‰ç±»åˆ«å’ŒæŠ¥é“æ•°é‡æ’åº
    trending_df = trending_df.sort_values(
        ['Category', 'SourceCount'], 
        ascending=[True, False]
    )
    
    # æ¯ä¸ªç±»åˆ«å– top N
    top_trending = []
    for category in trending_df['Category'].unique():
        category_trending = trending_df[trending_df['Category'] == category].head(top_n)
        top_trending.append(category_trending)
    
    if top_trending:
        return pd.concat(top_trending, ignore_index=True)
    else:
        return pd.DataFrame()

def format_trending_display(trending_df: pd.DataFrame) -> str:
    """æ ¼å¼åŒ–çƒ­ç‚¹æ¦œæ˜¾ç¤º"""
    if trending_df.empty:
        return "æš‚æ— çƒ­ç‚¹æ–°é—»"
    
    output = []
    current_category = None
    
    for _, row in trending_df.iterrows():
        if row['Category'] != current_category:
            current_category = row['Category']
            output.append(f"\n### ğŸ“Š {current_category}\n")
        
        output.append(f"**ğŸ”¥ {row['SourceCount']} å®¶åª’ä½“æŠ¥é“**: {row['Headline'][:100]}...")
        output.append(f"   - åª’ä½“: {row['Outlets']}")
        if row.get('Date'):
            output.append(f"   - æ—¥æœŸ: {row['Date']}")
        output.append("")
    
    return "\n".join(output)

