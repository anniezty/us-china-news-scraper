import streamlit as st
from datetime import date, datetime
import pandas as pd
import yaml, io
from utils import compile_or_regex
from openpyxl.utils import get_column_letter
import os

# Â∞ùËØïÂØºÂÖ• Google Sheets ÂäüËÉΩ
try:
    from google_sheets_integration import read_from_sheets, export_to_sheets
    from collector import collect as collect_rss
    HAS_SHEETS = True
except ImportError:
    HAS_SHEETS = False
    from collector import collect as collect_rss

st.set_page_config(page_title="U.S.-China News Scraper", layout="wide")

st.markdown("## U.S.-China News Scraper")

# Load config and categories
with open("config_en.yaml","r",encoding="utf-8") as f:
    CFG = yaml.safe_load(f) or {}
with open("categories_en.yaml","r",encoding="utf-8") as f:
    CATS = yaml.safe_load(f) or {}
CATEGORIES = CATS.get("categories", {})

# Source multiselect (domain keys from config)
all_sources = list(CFG.get("rss_feeds", {}).keys())
col1, col2 = st.columns([1,1])
with col1:
    start_date = st.date_input("Start date", value=date.today() - pd.Timedelta(days=7))
with col2:
    end_date = st.date_input("End date (<= today)", value=date.today(), min_value=date(2000,1,1), max_value=date.today())
selected_sources = st.multiselect("Sources (whitelist)", options=all_sources, default=all_sources)

# Google Sheets ÈÖçÁΩÆ
use_sheets_db = False
spreadsheet_id = None
if HAS_SHEETS:
    st.markdown("---")
    st.markdown("### üìä Êï∞ÊçÆÊù•Ê∫ê")
    use_sheets_db = st.checkbox("‰ªé Google Sheets ËØªÂèñÂéÜÂè≤Êï∞ÊçÆÔºàNYT, SCMP, ReutersÔºâ", value=True)
    if use_sheets_db:
        spreadsheet_id = st.text_input(
            "Google Sheets ID", 
            value=os.getenv("GOOGLE_SHEETS_ID", ""),
            placeholder="‰ªé Google Sheets URL ‰∏≠Ëé∑Âèñ",
            help="‰æãÂ¶Ç: https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit"
        )
        
        if spreadsheet_id:
            st.info("‚úÖ Â∞Ü‰ªé Google Sheets ËØªÂèñ NYT„ÄÅSCMP„ÄÅReuters ÁöÑÂéÜÂè≤Êï∞ÊçÆ")

run = st.button("Generate & Export", type="primary")

if run:
    if end_date > date.today():
        st.error("End date cannot be in the future.")
    elif start_date > end_date:
        st.error("Start date must be before end date.")
    else:
        with st.spinner("Collecting articles..."):
            # 1. ‰ªé Google Sheets ËØªÂèñÂéÜÂè≤Êï∞ÊçÆÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
            sheets_df = pd.DataFrame()
            if use_sheets_db and spreadsheet_id and HAS_SHEETS:
                try:
                    st.info("üìñ Ê≠£Âú®‰ªé Google Sheets ËØªÂèñÂéÜÂè≤Êï∞ÊçÆ...")
                    # Â∞ùËØïËØªÂèñÂ§ö‰∏™ÂèØËÉΩÁöÑ sheet
                    # ÁÆÄÂåñÔºöËØªÂèñÊâÄÊúâÊï∞ÊçÆÔºåÁÑ∂ÂêéËøáÊª§Êó•Êúü
                    # ÂÆûÈôÖÂèØ‰ª•‰ºòÂåñ‰∏∫Âè™ËØªÂèñÁõ∏ÂÖ≥ sheet
                    priority_sources = ["nytimes.com", "scmp.com", "reuters.com"]
                    
                    # Â∞ùËØïËØªÂèñÊâÄÊúâ‰ª• "Week" ÂºÄÂ§¥ÁöÑ sheetÔºåÂêàÂπ∂Êï∞ÊçÆ
                    try:
                        # ËØªÂèñÊâÄÊúâ Week sheet ÁöÑÊï∞ÊçÆ
                        import gspread
                        from google.oauth2.service_account import Credentials
                        from google_sheets_integration import get_sheets_client
                        
                        client = get_sheets_client(credentials_path=None)
                        spreadsheet = client.open_by_key(spreadsheet_id)
                        
                        all_sheets_data = []
                        for sheet in spreadsheet.worksheets():
                            # Âè™ËØªÂèñ‰ª• "Week" ÂºÄÂ§¥ÁöÑ sheet
                            if sheet.title.startswith("Week"):
                                try:
                                    data = sheet.get_all_values()
                                    if len(data) > 1:  # ÊúâÊï∞ÊçÆÔºàÊ†áÈ¢ò+Êï∞ÊçÆÔºâ
                                        df_part = pd.DataFrame(data[1:], columns=data[0])
                                        all_sheets_data.append(df_part)
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è ËØªÂèñ Sheet '{sheet.title}' Êó∂Âá∫Èîô: {e}")
                        
                        # ÂêàÂπ∂ÊâÄÊúâ sheet ÁöÑÊï∞ÊçÆ
                        if all_sheets_data:
                            sheets_df = pd.concat(all_sheets_data, ignore_index=True)
                        else:
                            sheets_df = pd.DataFrame()
                        
                        if not sheets_df.empty and 'Date' in sheets_df.columns:
                            # ËøáÊª§Êó•ÊúüËåÉÂõ¥
                            sheets_df['Date'] = pd.to_datetime(sheets_df['Date'], errors='coerce')
                            # Â§ÑÁêÜÊó•ÊúüËåÉÂõ¥ÔºöÂ¶ÇÊûúÂè™ÊúâÊó•ÊúüÔºàÊ≤°ÊúâÊó∂Èó¥ÔºâÔºåend_date Â∫îËØ•ÂåÖÂê´ÂΩìÂ§©ÁöÑÊâÄÊúâÊó∂Èó¥
                            date_from_dt = pd.to_datetime(start_date).normalize()  # ËÆæÁΩÆ‰∏∫ 00:00:00
                            date_to_dt = pd.to_datetime(end_date).normalize() + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # ËÆæÁΩÆ‰∏∫ 23:59:59
                            sheets_df = sheets_df[
                                (sheets_df['Date'] >= date_from_dt) & 
                                (sheets_df['Date'] <= date_to_dt)
                            ]
                            # Á°Æ‰øùÂàóÂêç‰∏ÄËá¥
                            if 'Nested?' not in sheets_df.columns:
                                sheets_df['Nested?'] = ''
                            st.success(f"‚úÖ ‰ªé Google Sheets ËØªÂèñ‰∫Ü {len(sheets_df)} Êù°ÂéÜÂè≤Êï∞ÊçÆ")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Êó†Ê≥ïËØªÂèñ Google Sheets: {e}")
                        sheets_df = pd.DataFrame()
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Google Sheets ËØªÂèñÂ§±Ë¥•: {e}")
            
            # 2. ‰ªé RSS ÂÆûÊó∂ÊäìÂèñÊâÄÊúâÊù•Ê∫ê
            st.info("üåê Ê≠£Âú®‰ªé RSS ÂÆûÊó∂ÊäìÂèñ...")
            rss_df = collect_rss(
                "config_en.yaml", 
                start_date.isoformat(), 
                end_date.isoformat(), 
                us_china_only=False, 
                limit_sources=selected_sources
            )
            
            # 3. ÂêàÂπ∂Êï∞ÊçÆ
            if not sheets_df.empty and not rss_df.empty:
                # Á°Æ‰øùÂàóÂêç‰∏ÄËá¥
                required_cols = ["Nested?","URL","Date","Outlet","Headline","Nut Graph"]
                for col in required_cols:
                    if col not in sheets_df.columns:
                        sheets_df[col] = ""
                    if col not in rss_df.columns:
                        rss_df[col] = ""
                
                # ÂêàÂπ∂
                df = pd.concat([sheets_df[required_cols], rss_df[required_cols]], ignore_index=True)
                # ÂéªÈáçÔºàÊåâ URLÔºâ
                df = df.drop_duplicates(subset=['URL'], keep='first')
                st.success(f"‚úÖ ÂêàÂπ∂ÂÆåÊàê: Google Sheets ({len(sheets_df)} Êù°) + RSS ({len(rss_df)} Êù°) = ÊÄªËÆ° {len(df)} Êù°ÔºàÂéªÈáçÂêéÔºâ")
            elif not sheets_df.empty:
                df = sheets_df
                st.success(f"‚úÖ ‰ΩøÁî® Google Sheets Êï∞ÊçÆ: {len(df)} Êù°")
            elif not rss_df.empty:
                df = rss_df
                st.success(f"‚úÖ ‰ΩøÁî® RSS Êï∞ÊçÆ: {len(rss_df)} Êù°")
            else:
                df = pd.DataFrame()
                st.warning("Êú™ÊâæÂà∞ÊñáÁ´†")

        if not df.empty:
            # Assign single category per article (first matched)
            compiled = []
            for cat, patt in CATEGORIES.items():
                try:
                    compiled.append((cat, compile_or_regex([patt])))
                except Exception:
                    continue

            def assign_category(row):
                # Â∞ùËØï‰ΩøÁî® API ÂàÜÁ±ªÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
                try:
                    from api_classifier import classify_with_api, is_api_available
                    if is_api_available():
                        category_list = [cat for cat, _ in compiled] + ["Uncategorized"]
                        api_cat = classify_with_api(
                            row.get('Headline', ''),
                            row.get('Nut Graph', ''),
                            category_list
                        )
                        if api_cat:
                            return api_cat
                except ImportError:
                    pass  # API ÂàÜÁ±ªÂô®Êú™ÂÆâË£ÖÔºå‰ΩøÁî®Ê≠£Âàô
                
                # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÂàÜÁ±ªÔºàÈªòËÆ§Ôºâ
                text = f"{row.get('Headline','')} || {row.get('Nut Graph','')}"
                for cat, rgx in compiled:
                    if rgx.search(text):
                        return cat
                return "Uncategorized"

            df = df.copy()
            df["Category"] = df.apply(assign_category, axis=1)

            # Per-category counts
            st.markdown("### Summary")
            st.write(f"- Total: **{len(df)}**")
            for cat, _ in compiled:
                sub = df[df["Category"] == cat]
                st.write(f"- {cat}: **{len(sub)}**")
            unc = df[df["Category"] == "Uncategorized"]
            if not unc.empty:
                st.write(f"- Uncategorized: **{len(unc)}**")

            # Build Excel in-memory
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
                # All first
                df.to_excel(writer, sheet_name="All", index=False)
                # Autofit for All
                ws_all = writer.sheets.get("All")
                if ws_all is not None:
                    for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                        if col_name in df.columns:
                            idx = list(df.columns).index(col_name) + 1
                            values = df[col_name].astype(str).tolist() if not df.empty else []
                            width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                            ws_all.column_dimensions[get_column_letter(idx)].width = max(width, 10)
                # Categories (single-category assignment)
                for cat, _ in compiled:
                    sub = df[df["Category"] == cat]
                    if not sub.empty:
                        sub = sub[["Nested?","URL","Date","Outlet","Headline","Nut Graph"]]
                        sheet = cat[:31]
                        sub.to_excel(writer, sheet_name=sheet, index=False)
                        ws = writer.sheets.get(sheet)
                        if ws is not None:
                            for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                                if col_name in sub.columns:
                                    idx = list(sub.columns).index(col_name) + 1
                                    values = sub[col_name].astype(str).tolist()
                                    width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                                    ws.column_dimensions[get_column_letter(idx)].width = max(width, 10)
                # Uncategorized
                if not unc.empty:
                    sub = unc[["Nested?","URL","Date","Outlet","Headline","Nut Graph"]]
                    sub.to_excel(writer, sheet_name="Uncategorized", index=False)
                    ws = writer.sheets.get("Uncategorized")
                    if ws is not None:
                        for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                            if col_name in sub.columns:
                                idx = list(sub.columns).index(col_name) + 1
                                values = sub[col_name].astype(str).tolist()
                                width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                                ws.column_dimensions[get_column_letter(idx)].width = max(width, 10)

            buffer.seek(0)
            default_name = f"us_china_news_{start_date}_{end_date}.xlsx"
            st.download_button("‚¨áÔ∏è Download Excel", data=buffer, file_name=default_name, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

