import streamlit as st
from datetime import date, datetime
import pandas as pd
import yaml, io
from utils import compile_or_regex
from openpyxl.utils import get_column_letter
import os

# å°è¯•å¯¼å…¥ Google Sheets åŠŸèƒ½
try:
    from google_sheets_integration import read_from_sheets, export_to_sheets
    from collector import collect as collect_rss
    HAS_SHEETS = True
except ImportError:
    HAS_SHEETS = False
    from collector import collect as collect_rss

st.set_page_config(page_title="U.S.-China News Scraper", layout="wide")

st.markdown("## U.S.-China News Scraper")

# Load config and categories
with open("config_en.yaml","r",encoding="utf-8") as f:
    CFG = yaml.safe_load(f) or {}
with open("categories_en.yaml","r",encoding="utf-8") as f:
    CATS = yaml.safe_load(f) or {}
CATEGORIES = CATS.get("categories", {})

# Source multiselect (domain keys from config)
all_sources = list(CFG.get("rss_feeds", {}).keys())
col1, col2 = st.columns([1,1])
with col1:
    start_date = st.date_input("Start date", value=date.today() - pd.Timedelta(days=7))
with col2:
    end_date = st.date_input("End date (<= today)", value=date.today(), min_value=date(2000,1,1), max_value=date.today())
selected_sources = st.multiselect("Sources (whitelist)", options=all_sources, default=all_sources)

# Google Sheets é…ç½®
use_sheets_db = False
spreadsheet_id = None
if HAS_SHEETS:
    st.markdown("---")
    st.markdown("### ğŸ“Š æ•°æ®æ¥æº")
    use_sheets_db = st.checkbox("ä» Google Sheets è¯»å–å†å²æ•°æ®ï¼ˆNYT, SCMP, Reutersï¼‰", value=True)
    if use_sheets_db:
        spreadsheet_id = st.text_input(
            "Google Sheets ID", 
            value=os.getenv("GOOGLE_SHEETS_ID", ""),
            placeholder="ä» Google Sheets URL ä¸­è·å–",
            help="ä¾‹å¦‚: https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit"
        )
        
        if spreadsheet_id:
            st.info("âœ… å°†ä» Google Sheets è¯»å– NYTã€SCMPã€Reuters çš„å†å²æ•°æ®")

run = st.button("Generate & Export", type="primary")

if run:
    if end_date > date.today():
        st.error("End date cannot be in the future.")
    elif start_date > end_date:
        st.error("Start date must be before end date.")
    else:
        with st.spinner("Collecting articles..."):
            # 1. ä» Google Sheets è¯»å–å†å²æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            sheets_df = pd.DataFrame()
            if use_sheets_db and spreadsheet_id and HAS_SHEETS:
                try:
                    st.info("ğŸ“– æ­£åœ¨ä» Google Sheets è¯»å–å†å²æ•°æ®...")
                    # å°è¯•è¯»å–å¤šä¸ªå¯èƒ½çš„ sheet
                    # ç®€åŒ–ï¼šè¯»å–æ‰€æœ‰æ•°æ®ï¼Œç„¶åè¿‡æ»¤æ—¥æœŸ
                    # å®é™…å¯ä»¥ä¼˜åŒ–ä¸ºåªè¯»å–ç›¸å…³ sheet
                    priority_sources = ["nytimes.com", "scmp.com", "reuters.com"]
                    
                    # å°è¯•è¯»å–æ‰€æœ‰ä»¥ "Week" å¼€å¤´çš„ sheetï¼Œåˆå¹¶æ•°æ®
                    try:
                        # è¯»å–æ‰€æœ‰ Week sheet çš„æ•°æ®
                        import gspread
                        from google.oauth2.service_account import Credentials
                        from google_sheets_integration import get_sheets_client
                        
                        client = get_sheets_client(credentials_path=None)
                        spreadsheet = client.open_by_key(spreadsheet_id)
                        
                        all_sheets_data = []
                        for sheet in spreadsheet.worksheets():
                            # åªè¯»å–ä»¥ "Week" å¼€å¤´çš„ sheet
                            if sheet.title.startswith("Week"):
                                try:
                                    data = sheet.get_all_values()
                                    if len(data) > 1:  # æœ‰æ•°æ®ï¼ˆæ ‡é¢˜+æ•°æ®ï¼‰
                                        df_part = pd.DataFrame(data[1:], columns=data[0])
                                        all_sheets_data.append(df_part)
                                except Exception as e:
                                    st.warning(f"âš ï¸ è¯»å– Sheet '{sheet.title}' æ—¶å‡ºé”™: {e}")
                        
                        # åˆå¹¶æ‰€æœ‰ sheet çš„æ•°æ®
                        if all_sheets_data:
                            sheets_df = pd.concat(all_sheets_data, ignore_index=True)
                        else:
                            sheets_df = pd.DataFrame()
                        
                        if not sheets_df.empty and 'Date' in sheets_df.columns:
                            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                            sheets_df['Date'] = pd.to_datetime(sheets_df['Date'], errors='coerce')
                            # å¤„ç†æ—¥æœŸèŒƒå›´ï¼šå¦‚æœåªæœ‰æ—¥æœŸï¼ˆæ²¡æœ‰æ—¶é—´ï¼‰ï¼Œend_date åº”è¯¥åŒ…å«å½“å¤©çš„æ‰€æœ‰æ—¶é—´
                            date_from_dt = pd.to_datetime(start_date).normalize()  # è®¾ç½®ä¸º 00:00:00
                            date_to_dt = pd.to_datetime(end_date).normalize() + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # è®¾ç½®ä¸º 23:59:59
                            sheets_df = sheets_df[
                                (sheets_df['Date'] >= date_from_dt) & 
                                (sheets_df['Date'] <= date_to_dt)
                            ]
                            # ç¡®ä¿åˆ—åä¸€è‡´
                            if 'Nested?' not in sheets_df.columns:
                                sheets_df['Nested?'] = ''
                            st.success(f"âœ… ä» Google Sheets è¯»å–äº† {len(sheets_df)} æ¡å†å²æ•°æ®")
                    except Exception as e:
                        st.warning(f"âš ï¸ æ— æ³•è¯»å– Google Sheets: {e}")
                        sheets_df = pd.DataFrame()
                except Exception as e:
                    st.warning(f"âš ï¸ Google Sheets è¯»å–å¤±è´¥: {e}")
            
            # 2. ä» RSS å®æ—¶æŠ“å–æ‰€æœ‰æ¥æº
            st.info("ğŸŒ æ­£åœ¨ä» RSS å®æ—¶æŠ“å–...")
            rss_df = collect_rss(
                "config_en.yaml", 
                start_date.isoformat(), 
                end_date.isoformat(), 
                us_china_only=False, 
                limit_sources=selected_sources
            )
            
            # 3. åˆå¹¶æ•°æ®
            if not sheets_df.empty and not rss_df.empty:
                # ç¡®ä¿åˆ—åä¸€è‡´
                required_cols = ["Nested?","URL","Date","Outlet","Headline","Nut Graph"]
                for col in required_cols:
                    if col not in sheets_df.columns:
                        sheets_df[col] = ""
                    if col not in rss_df.columns:
                        rss_df[col] = ""
                
                # åˆå¹¶
                df = pd.concat([sheets_df[required_cols], rss_df[required_cols]], ignore_index=True)
                # å»é‡ï¼ˆæŒ‰ URLï¼‰
                df = df.drop_duplicates(subset=['URL'], keep='first')
                st.success(f"âœ… åˆå¹¶å®Œæˆ: Google Sheets ({len(sheets_df)} æ¡) + RSS ({len(rss_df)} æ¡) = æ€»è®¡ {len(df)} æ¡ï¼ˆå»é‡åï¼‰")
            elif not sheets_df.empty:
                df = sheets_df
                st.success(f"âœ… ä½¿ç”¨ Google Sheets æ•°æ®: {len(df)} æ¡")
            elif not rss_df.empty:
                df = rss_df
                st.success(f"âœ… ä½¿ç”¨ RSS æ•°æ®: {len(rss_df)} æ¡")
            else:
                df = pd.DataFrame()
                st.warning("æœªæ‰¾åˆ°æ–‡ç« ")

        if not df.empty:
            # Assign single category per article (first matched)
            compiled = []
            for cat, patt in CATEGORIES.items():
                try:
                    compiled.append((cat, compile_or_regex([patt])))
                except Exception:
                    continue

            def assign_category(row):
                # å°è¯•ä½¿ç”¨ API åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    from api_classifier import classify_with_api, is_api_available
                    if is_api_available():
                        category_list = [cat for cat, _ in compiled] + ["Uncategorized"]
                        api_cat = classify_with_api(
                            row.get('Headline', ''),
                            row.get('Nut Graph', ''),
                            category_list
                        )
                        if api_cat:
                            return api_cat
                except ImportError:
                    pass  # API åˆ†ç±»å™¨æœªå®‰è£…ï¼Œä½¿ç”¨æ­£åˆ™
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†ç±»ï¼ˆé»˜è®¤ï¼‰
                text = f"{row.get('Headline','')} || {row.get('Nut Graph','')}"
                for cat, rgx in compiled:
                    if rgx.search(text):
                        return cat
                return "Uncategorized"

            df = df.copy()
            df["Category"] = df.apply(assign_category, axis=1)

            # Per-category counts - ä½¿ç”¨ä¸¤åˆ—å¸ƒå±€æ›´ç›´è§‚
            st.markdown("### ğŸ“Š Summary")
            
            # è®¡ç®—æ€»æ•°
            total = len(df)
            unc = df[df["Category"] == "Uncategorized"]
            unc_count = len(unc) if not unc.empty else 0
            
            # åªæ˜¾ç¤ºä¸¤ä¸ªå…³é”®æŒ‡æ ‡
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("ğŸ“° Total Articles", total)
            
            with col2:
                st.metric("ğŸ“‚ Categories", len(compiled))
            
            # æŒ‰ç±»åˆ«æ˜¾ç¤ºç»Ÿè®¡ï¼ˆä½¿ç”¨ä¸¤åˆ—ï¼‰
            st.markdown("---")
            st.markdown("#### ğŸ“‹ By Category")
            
            # æŒ‰æ•°é‡æ’åº
            category_counts = []
            for cat, _ in compiled:
                sub = df[df["Category"] == cat]
                category_counts.append((cat, len(sub)))
            category_counts.sort(key=lambda x: x[1], reverse=True)
            
            # ä¸¤åˆ—æ˜¾ç¤º
            cols = st.columns(2)
            for idx, (cat, count) in enumerate(category_counts):
                col_idx = idx % 2
                with cols[col_idx]:
                    # è®¡ç®—ç™¾åˆ†æ¯”
                    percentage = (count / total * 100) if total > 0 else 0
                    # ä½¿ç”¨è¿›åº¦æ¡æ›´ç›´è§‚
                    st.markdown(f"**{cat}**")
                    st.progress(min(count / total, 1.0) if total > 0 else 0)
                    st.caption(f"{count} articles ({percentage:.1f}%)")
            
            # Uncategorized å•ç‹¬æ˜¾ç¤º
            if unc_count > 0:
                st.markdown("---")
                st.markdown(f"**Uncategorized**: {unc_count} articles ({(unc_count/total*100):.1f}%)")
                st.progress(min(unc_count / total, 1.0) if total > 0 else 0)
            
            # çƒ­ç‚¹æ¦œåŠŸèƒ½
            st.markdown("---")
            st.markdown("## ğŸ”¥ çƒ­ç‚¹æ¦œ")
            st.markdown("æ˜¾ç¤ºè¢«å¤šå®¶åª’ä½“æŠ¥é“çš„æ–°é—»ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰")
            
            try:
                from news_trending import group_similar_news, generate_trending_rank
                
                # è¯†åˆ«ç›¸ä¼¼æ–°é—»å¹¶åˆ†ç»„
                with st.spinner("æ­£åœ¨åˆ†ææ–°é—»çƒ­ç‚¹..."):
                    df_with_groups = group_similar_news(df.copy(), similarity_threshold=0.6)
                    
                    # ç”Ÿæˆçƒ­ç‚¹æ¦œ
                    trending_df = generate_trending_rank(df_with_groups, top_n=3)
                    
                    if not trending_df.empty:
                        # è·å–æ‰€æœ‰ç±»åˆ«
                        categories = sorted(trending_df['Category'].unique())
                        
                        # ä½¿ç”¨ tabs è®©ç”¨æˆ·é€‰æ‹©ç±»åˆ«
                        if len(categories) > 1:
                            tabs = st.tabs(categories)
                            for idx, category in enumerate(categories):
                                with tabs[idx]:
                                    category_trending = trending_df[trending_df['Category'] == category]
                                    
                                    for _, row in category_trending.iterrows():
                                        with st.container():
                                            st.markdown(f"### ğŸ”¥ {row['SourceCount']} å®¶åª’ä½“æŠ¥é“")
                                            st.markdown(f"**{row['Headline']}**")
                                            st.markdown(f"**æŠ¥é“åª’ä½“**: {row['Outlets']}")
                                            if row.get('Date'):
                                                st.markdown(f"**æ—¥æœŸ**: {row['Date']}")
                                            
                                            # æ˜¾ç¤ºæ‰€æœ‰é“¾æ¥
                                            if row.get('URLs') and len(row['URLs']) > 0:
                                                st.markdown("**ç›¸å…³æŠ¥é“**:")
                                                for url in row['URLs'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé“¾æ¥
                                                    st.markdown(f"- [æŸ¥çœ‹åŸæ–‡]({url})")
                                            
                                            st.markdown("---")
                        else:
                            # åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œç›´æ¥æ˜¾ç¤º
                            category = categories[0]
                            category_trending = trending_df[trending_df['Category'] == category]
                            
                            for _, row in category_trending.iterrows():
                                with st.container():
                                    st.markdown(f"### ğŸ”¥ {row['SourceCount']} å®¶åª’ä½“æŠ¥é“")
                                    st.markdown(f"**{row['Headline']}**")
                                    st.markdown(f"**æŠ¥é“åª’ä½“**: {row['Outlets']}")
                                    if row.get('Date'):
                                        st.markdown(f"**æ—¥æœŸ**: {row['Date']}")
                                    
                                    # æ˜¾ç¤ºæ‰€æœ‰é“¾æ¥
                                    if row.get('URLs') and len(row['URLs']) > 0:
                                        st.markdown("**ç›¸å…³æŠ¥é“**:")
                                        for url in row['URLs'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé“¾æ¥
                                            st.markdown(f"- [æŸ¥çœ‹åŸæ–‡]({url})")
                                    
                                    st.markdown("---")
                    else:
                        st.info("æš‚æ— çƒ­ç‚¹æ–°é—»ï¼ˆéœ€è¦è‡³å°‘2å®¶åª’ä½“æŠ¥é“åŒä¸€æ–°é—»ï¼‰")
            except ImportError as e:
                st.warning(f"âš ï¸ çƒ­ç‚¹æ¦œåŠŸèƒ½æš‚ä¸å¯ç”¨: {e}")
            except Exception as e:
                st.warning(f"âš ï¸ ç”Ÿæˆçƒ­ç‚¹æ¦œæ—¶å‡ºé”™: {e}")
                import traceback
                st.code(traceback.format_exc())
 
            # Build Excel in-memory
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
                # All first
                df.to_excel(writer, sheet_name="All", index=False)
                # Autofit for All
                ws_all = writer.sheets.get("All")
                if ws_all is not None:
                    for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                        if col_name in df.columns:
                            idx = list(df.columns).index(col_name) + 1
                            values = df[col_name].astype(str).tolist() if not df.empty else []
                            width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                            ws_all.column_dimensions[get_column_letter(idx)].width = max(width, 10)
                # Categories (single-category assignment)
                for cat, _ in compiled:
                    sub = df[df["Category"] == cat]
                    if not sub.empty:
                        sub = sub[["Nested?","URL","Date","Outlet","Headline","Nut Graph"]]
                        sheet = cat[:31]
                        sub.to_excel(writer, sheet_name=sheet, index=False)
                        ws = writer.sheets.get(sheet)
                        if ws is not None:
                            for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                                if col_name in sub.columns:
                                    idx = list(sub.columns).index(col_name) + 1
                                    values = sub[col_name].astype(str).tolist()
                                    width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                                    ws.column_dimensions[get_column_letter(idx)].width = max(width, 10)
                # Uncategorized
                if not unc.empty:
                    sub = unc[["Nested?","URL","Date","Outlet","Headline","Nut Graph"]]
                    sub.to_excel(writer, sheet_name="Uncategorized", index=False)
                    ws = writer.sheets.get("Uncategorized")
                    if ws is not None:
                        for col_name, max_width in [("Date", 22), ("Outlet", 18), ("Headline", 80)]:
                            if col_name in sub.columns:
                                idx = list(sub.columns).index(col_name) + 1
                                values = sub[col_name].astype(str).tolist()
                                width = min(max(len(col_name), max((len(v) for v in values), default=0)) + 2, max_width)
                                ws.column_dimensions[get_column_letter(idx)].width = max(width, 10)

            buffer.seek(0)
            default_name = f"us_china_news_{start_date}_{end_date}.xlsx"
            st.download_button("â¬‡ï¸ Download Excel", data=buffer, file_name=default_name, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

