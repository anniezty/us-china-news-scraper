#!/usr/bin/env python3
"""
Google Sheets é›†æˆ
è‡ªåŠ¨å°†æ•°æ®å¯¼å‡ºåˆ° Google Sheets
"""
import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
from datetime import datetime, timedelta
import yaml
import os
import json

# Google Sheets API æƒé™èŒƒå›´
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

def get_sheets_client(credentials_path: str = None):
    """
    è·å– Google Sheets å®¢æˆ·ç«¯
    
    æ”¯æŒå¤šç§å‡­è¯æ¥æºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. Streamlit Secretsï¼ˆå¦‚æœåœ¨ Streamlit ç¯å¢ƒä¸­ï¼‰
    2. ç¯å¢ƒå˜é‡ GOOGLE_CREDENTIALS_JSONï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
    3. æœ¬åœ°æ–‡ä»¶ google_credentials.json
    """
    creds = None
    
    # æ–¹å¼ 1: å°è¯•ä» Streamlit Secrets è¯»å–ï¼ˆä»…åœ¨ Streamlit ç¯å¢ƒä¸­ï¼‰
    try:
        import streamlit as st
        # åœ¨ Streamlit ç¯å¢ƒä¸­ï¼Œç›´æ¥æ£€æŸ¥ secrets
        if hasattr(st, 'secrets'):
            try:
                if 'google_sheets' in st.secrets:
                    creds_dict = st.secrets['google_sheets'].get('credentials')
                    if creds_dict:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ JSON
                        if isinstance(creds_dict, str):
                            creds_dict = json.loads(creds_dict)
                        elif isinstance(creds_dict, dict):
                            # å·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                            pass
                        else:
                            creds_dict = None
                        
                        if creds_dict:
                            creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
                            return gspread.authorize(creds)
            except (KeyError, AttributeError, json.JSONDecodeError) as e:
                # Secrets é…ç½®æœ‰é—®é¢˜ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹å¼
                pass
    except (ImportError, FileNotFoundError):
        # ä¸åœ¨ Streamlit ç¯å¢ƒæˆ– secrets æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹å¼
        pass
    
    # æ–¹å¼ 2: å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
    creds_json = os.getenv("GOOGLE_CREDENTIALS_JSON")
    if creds_json:
        try:
            creds_dict = json.loads(creds_json)
            creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
            return gspread.authorize(creds)
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"âš ï¸ ç¯å¢ƒå˜é‡ GOOGLE_CREDENTIALS_JSON æ ¼å¼é”™è¯¯: {e}")
    
    # æ–¹å¼ 3: ä»æœ¬åœ°æ–‡ä»¶è¯»å–
    # å¦‚æœ credentials_path ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if credentials_path is None:
        credentials_path = "google_credentials.json"
    
    if credentials_path and os.path.exists(credentials_path):
        creds = Credentials.from_service_account_file(credentials_path, scopes=SCOPES)
        return gspread.authorize(creds)
    
    # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
    raise FileNotFoundError(
        f"Google å‡­è¯æœªæ‰¾åˆ°\n"
        "è¯·é€‰æ‹©ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€ï¼š\n"
        "1. åœ¨ Streamlit Secrets ä¸­é…ç½® google_sheets.credentials\n"
        "2. è®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_CREDENTIALS_JSON\n"
        "3. åˆ›å»ºæ–‡ä»¶ google_credentials.json"
    )

def export_to_sheets(df: pd.DataFrame, spreadsheet_id: str, sheet_name: str = None, 
                     credentials_path: str = None):
    """
    å¯¼å‡º DataFrame åˆ° Google Sheets
    
    Args:
        df: è¦å¯¼å‡ºçš„ DataFrame
        spreadsheet_id: Google Sheets çš„ IDï¼ˆä» URL ä¸­è·å–ï¼‰
        sheet_name: Sheet åç§°ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è¦†ç›–ç¬¬ä¸€ä¸ª sheetï¼‰
        credentials_path: Google å‡­è¯æ–‡ä»¶è·¯å¾„
    """
    client = get_sheets_client(credentials_path)
    
    # æ‰“å¼€ spreadsheet
    spreadsheet = client.open_by_key(spreadsheet_id)
    
    # é€‰æ‹©æˆ–åˆ›å»º sheet
    if sheet_name:
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=10)
    else:
        worksheet = spreadsheet.sheet1
    
    # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆä¿ç•™æ ‡é¢˜è¡Œï¼‰
    worksheet.clear()
    
    # å†™å…¥æ•°æ®
    # å…ˆå†™å…¥åˆ—å
    worksheet.append_row(df.columns.tolist())
    
    # å†™å…¥æ•°æ®ï¼ˆåˆ†æ‰¹å†™å…¥ï¼Œé¿å…è¶…æ—¶ï¼‰
    batch_size = 100
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        values = batch.values.tolist()
        worksheet.append_rows(values)
    
    print(f"âœ… å·²å¯¼å‡º {len(df)} è¡Œæ•°æ®åˆ° Google Sheets: {sheet_name}")

def export_to_sheets_append(df: pd.DataFrame, spreadsheet_id: str, sheet_name: str = None, 
                            credentials_path: str = None, sort_by_date: bool = True):
    """
    è¿½åŠ  DataFrame åˆ° Google Sheetsï¼ˆè·¨ sheet å»é‡åè¿½åŠ ï¼Œå¹¶æŒ‰æ—¥æœŸæ’åºï¼‰
    
    Args:
        df: è¦è¿½åŠ çš„ DataFrame
        spreadsheet_id: Google Sheets çš„ ID
        sheet_name: Sheet åç§°
        credentials_path: Google å‡­è¯æ–‡ä»¶è·¯å¾„
        sort_by_date: æ˜¯å¦æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
    """
    client = get_sheets_client(credentials_path)
    spreadsheet = client.open_by_key(spreadsheet_id)
    
    # è·¨ sheet å»é‡ï¼šæ”¶é›†æ‰€æœ‰ sheet ä¸­çš„ URLï¼ˆç”¨äºé˜²æ­¢é‡å¤æ·»åŠ æ–°æ•°æ®ï¼‰
    # æ³¨æ„ï¼šæ’é™¤å½“å‰ sheetï¼Œå› ä¸ºæˆ‘ä»¬è¦åˆå¹¶åˆ°å½“å‰ sheetï¼Œä¼šåœ¨åˆå¹¶åå•ç‹¬å¤„ç†å½“å‰ sheet çš„å»é‡
    all_existing_urls = set()
    current_sheet_urls = set()  # å½“å‰ sheet çš„ URLï¼ˆç”¨äºåç»­åˆå¹¶æ—¶å»é‡ï¼‰
    if 'URL' in df.columns:
        for sheet in spreadsheet.worksheets():
            try:
                sheet_data = sheet.get_all_values()
                # è·³è¿‡ç©ºç™½ sheetï¼ˆåªæœ‰æ ‡é¢˜è¡Œæˆ–å®Œå…¨æ²¡æœ‰æ•°æ®ï¼‰
                if len(sheet_data) <= 1:
                    continue
                # ç¡®ä¿æœ‰æ ‡é¢˜è¡Œä¸”è‡³å°‘æœ‰ä¸€è¡Œæ•°æ®
                if len(sheet_data[0]) == 0:
                    continue
                sheet_df = pd.DataFrame(sheet_data[1:], columns=sheet_data[0])
                # ç¡®ä¿æœ‰ URL åˆ—ä¸”æœ‰å®é™…æ•°æ®
                if 'URL' not in sheet_df.columns:
                    continue
                urls = sheet_df['URL'].dropna()
                # å¦‚æœæ²¡æœ‰ä»»ä½• URLï¼Œè·³è¿‡ï¼ˆå¯èƒ½æ˜¯ç©ºç™½ sheetï¼‰
                if len(urls) == 0:
                    continue
                # å¦‚æœæ˜¯å½“å‰ sheetï¼Œå•ç‹¬è®°å½•ï¼ˆç”¨äºåç»­åˆå¹¶æ—¶å»é‡ï¼‰
                if sheet_name and sheet.title == sheet_name:
                    current_sheet_urls.update(urls)
                else:
                    # å…¶ä»– sheet çš„ URL ç”¨äºè·¨ sheet å»é‡
                    all_existing_urls.update(urls)
            except Exception as e:
                print(f"âš ï¸ è¯»å– sheet '{sheet.title}' æ—¶å‡ºé”™: {e}")
                continue
        
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„ URLï¼ˆåªè¿‡æ»¤æ–°æ•°æ®ï¼Œä¸å½±å“ç°æœ‰æ•°æ®ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œåªè¿‡æ»¤å…¶ä»– sheet çš„ URLï¼Œä¸åŒ…å«å½“å‰ sheet
        original_count = len(df)
        df = df[~df['URL'].isin(all_existing_urls)]
        if len(df) < original_count:
            print(f"ğŸ“ è·¨ sheet å»é‡ï¼šè¿‡æ»¤æ‰ {original_count - len(df)} ç¯‡å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆå…¶ä»– sheet ä¸­ï¼‰")
    
    # é€‰æ‹©æˆ–åˆ›å»º sheet
    if sheet_name:
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            # å¦‚æœ sheet å·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®
            existing_data = worksheet.get_all_values()
            if len(existing_data) > 1:
                original_row_count = len(existing_data) - 1  # å‡å»æ ‡é¢˜è¡Œ
                existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
                
                # ä¿æŠ¤æªæ–½ï¼šæ£€æŸ¥è¯»å–çš„æ•°æ®é‡
                if len(existing_df) != original_row_count:
                    print(f"âš ï¸ è­¦å‘Šï¼šè¯»å–çš„æ•°æ®è¡Œæ•°ä¸åŒ¹é…ï¼æœŸæœ› {original_row_count} è¡Œï¼Œå®é™… {len(existing_df)} è¡Œ")
                
                # ç¡®ä¿åˆ—åå’Œé¡ºåºåŒ¹é…ï¼Œé¿å…æ•°æ®ä¸¢å¤±
                # ä½¿ç”¨æ–°æ•°æ®çš„åˆ—åå’Œé¡ºåºä½œä¸ºæ ‡å‡†
                expected_columns = df.columns.tolist()
                # å¦‚æœç°æœ‰æ•°æ®çš„åˆ—åä¸åŒ¹é…ï¼Œå°è¯•å¯¹é½
                if list(existing_df.columns) != expected_columns:
                    print(f"âš ï¸ åˆ—åä¸åŒ¹é…ï¼ç°æœ‰åˆ—: {list(existing_df.columns)}, æœŸæœ›åˆ—: {expected_columns}")
                    # è®°å½•åŸå§‹è¡Œæ•°
                    before_alignment = len(existing_df)
                    # å°è¯•é‡æ–°å¯¹é½åˆ—
                    existing_df_aligned = pd.DataFrame()
                    for col in expected_columns:
                        if col in existing_df.columns:
                            existing_df_aligned[col] = existing_df[col]
                        else:
                            existing_df_aligned[col] = None  # ç¼ºå¤±çš„åˆ—å¡«å……ä¸º None
                    existing_df = existing_df_aligned
                    # ä¿æŠ¤æªæ–½ï¼šæ£€æŸ¥å¯¹é½åè¡Œæ•°æ˜¯å¦ä¸€è‡´
                    if len(existing_df) != before_alignment:
                        print(f"âš ï¸ è­¦å‘Šï¼šåˆ—å¯¹é½åè¡Œæ•°å˜åŒ–ï¼å¯¹é½å‰ {before_alignment} è¡Œï¼Œå¯¹é½å {len(existing_df)} è¡Œ")
                        # å¦‚æœè¡Œæ•°å‡å°‘ï¼Œå°è¯•æ¢å¤
                        if len(existing_df) < before_alignment:
                            print(f"âŒ é”™è¯¯ï¼šåˆ—å¯¹é½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼åœæ­¢æ“ä½œï¼Œä¸æ¸…ç©º sheet")
                            raise ValueError(f"åˆ—å¯¹é½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼š{before_alignment} è¡Œ -> {len(existing_df)} è¡Œ")
                
                # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´ï¼ˆä¸ä¼šå¯¼è‡´è¡Œæ•°å‡å°‘ï¼‰
                existing_df = existing_df[expected_columns]
                
                # ä¿æŠ¤æªæ–½ï¼šæœ€ç»ˆæ£€æŸ¥
                if len(existing_df) != original_row_count:
                    print(f"âŒ é”™è¯¯ï¼šæ•°æ®å¤„ç†åè¡Œæ•°ä¸åŒ¹é…ï¼åŸå§‹ {original_row_count} è¡Œï¼Œå¤„ç†å {len(existing_df)} è¡Œ")
                    raise ValueError(f"æ•°æ®å¤„ç†å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼š{original_row_count} è¡Œ -> {len(existing_df)} è¡Œ")
                # åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®
                # æ³¨æ„ï¼šdf å·²ç»è·¨ sheet å»é‡ï¼ˆä¸åŒ…å«å…¶ä»– sheet çš„ URLï¼‰ï¼Œä½†å¯èƒ½åŒ…å«å½“å‰ sheet çš„ URL
                # æ‰€ä»¥éœ€è¦è¿‡æ»¤æ‰æ–°æ•°æ®ä¸­å·²åœ¨å½“å‰ sheet å­˜åœ¨çš„ URL
                if current_sheet_urls and 'URL' in df.columns:
                    before_filter = len(df)
                    df = df[~df['URL'].isin(current_sheet_urls)]
                    if len(df) < before_filter:
                        print(f"ğŸ“ å½“å‰ sheet å»é‡ï¼šè¿‡æ»¤æ‰ {before_filter - len(df)} ç¯‡å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆå½“å‰ sheet ä¸­ï¼‰")
                
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                # åœ¨å½“å‰ sheet å†…å»é‡ï¼ˆæ¸…ç† existing_df æœ¬èº«å¯èƒ½å­˜åœ¨çš„é‡å¤ï¼‰
                if 'URL' in combined_df.columns:
                    before_dedup = len(combined_df)
                    # æ”¹è¿›ï¼šæ¸…ç† URL æ ¼å¼ï¼ˆå»é™¤ç©ºæ ¼ã€ç»Ÿä¸€æ ¼å¼ï¼‰é¿å…è¯¯åˆ¤ä¸ºé‡å¤
                    combined_df['URL_cleaned'] = combined_df['URL'].astype(str).str.strip()
                    # ä½¿ç”¨æ¸…ç†åçš„ URL å»é‡
                    combined_df = combined_df.drop_duplicates(subset=['URL_cleaned'], keep='first')
                    # åˆ é™¤ä¸´æ—¶åˆ—
                    combined_df = combined_df.drop('URL_cleaned', axis=1)
                    after_dedup = len(combined_df)
                    if before_dedup > after_dedup:
                        print(f"ğŸ“ Sheet å†…å»é‡ï¼šç§»é™¤ {before_dedup - after_dedup} ç¯‡é‡å¤æ–‡ç« ï¼ˆæ¸…ç†ç°æœ‰æ•°æ®ä¸­çš„é‡å¤ï¼‰")
            else:
                # sheet å­˜åœ¨ä½†åªæœ‰æ ‡é¢˜è¡Œï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                combined_df = df.copy()
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=10)
            # æ–° sheetï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
            combined_df = df.copy()
            existing_data = []  # æ–° sheetï¼Œæ²¡æœ‰ç°æœ‰æ•°æ®
    else:
        worksheet = spreadsheet.sheet1
        existing_data = worksheet.get_all_values()
        if len(existing_data) > 1:
            existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
            # ç¡®ä¿åˆ—åå’Œé¡ºåºåŒ¹é…
            expected_columns = df.columns.tolist()
            if list(existing_df.columns) != expected_columns:
                print(f"âš ï¸ åˆ—åä¸åŒ¹é…ï¼ç°æœ‰åˆ—: {list(existing_df.columns)}, æœŸæœ›åˆ—: {expected_columns}")
                existing_df_aligned = pd.DataFrame()
                for col in expected_columns:
                    if col in existing_df.columns:
                        existing_df_aligned[col] = existing_df[col]
                    else:
                        existing_df_aligned[col] = None
                existing_df = existing_df_aligned
            existing_df = existing_df[expected_columns]
            # åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®
            # æ³¨æ„ï¼šdf å·²ç»è·¨ sheet å»é‡ï¼Œä½†å¯èƒ½åŒ…å«å½“å‰ sheet çš„ URL
            # éœ€è¦è¿‡æ»¤æ‰æ–°æ•°æ®ä¸­å·²åœ¨å½“å‰ sheet å­˜åœ¨çš„ URL
            if 'URL' in df.columns:
                # ä» existing_df ä¸­è·å–å½“å‰ sheet çš„ URL
                current_sheet_urls_from_existing = set(existing_df['URL'].dropna()) if 'URL' in existing_df.columns else set()
                before_filter = len(df)
                df = df[~df['URL'].isin(current_sheet_urls_from_existing)]
                if len(df) < before_filter:
                    print(f"ğŸ“ å½“å‰ sheet å»é‡ï¼šè¿‡æ»¤æ‰ {before_filter - len(df)} ç¯‡å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆå½“å‰ sheet ä¸­ï¼‰")
            
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            # åœ¨å½“å‰ sheet å†…å»é‡ï¼ˆæ¸…ç† existing_df æœ¬èº«å¯èƒ½å­˜åœ¨çš„é‡å¤ï¼‰
            if 'URL' in combined_df.columns:
                before_dedup = len(combined_df)
                # æ”¹è¿›ï¼šæ¸…ç† URL æ ¼å¼ï¼ˆå»é™¤ç©ºæ ¼ã€ç»Ÿä¸€æ ¼å¼ï¼‰é¿å…è¯¯åˆ¤ä¸ºé‡å¤
                combined_df['URL_cleaned'] = combined_df['URL'].astype(str).str.strip()
                # ä½¿ç”¨æ¸…ç†åçš„ URL å»é‡
                combined_df = combined_df.drop_duplicates(subset=['URL_cleaned'], keep='first')
                # åˆ é™¤ä¸´æ—¶åˆ—
                combined_df = combined_df.drop('URL_cleaned', axis=1)
                after_dedup = len(combined_df)
                if before_dedup > after_dedup:
                    print(f"ğŸ“ Sheet å†…å»é‡ï¼šç§»é™¤ {before_dedup - after_dedup} ç¯‡é‡å¤æ–‡ç« ï¼ˆæ¸…ç†ç°æœ‰æ•°æ®ä¸­çš„é‡å¤ï¼‰")
        else:
            combined_df = df.copy()
    
    # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œåªé‡æ–°æ’åº
    if df.empty:
        if len(existing_data) > 1:
            existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])
            print(f"âš ï¸ æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼ˆè·¨ sheet å»é‡ï¼‰ï¼Œé‡æ–°æ’åºç°æœ‰æ•°æ®...")
            if sort_by_date and 'Date' in existing_df.columns:
                _sort_sheet_by_date(worksheet, existing_df, existing_data[0])
                print(f"âœ… å·²æŒ‰æ—¥æœŸæ’åºå®Œæˆ")
            return
        else:
            print(f"âš ï¸ æ²¡æœ‰æ–°æ•°æ®å¯è¿½åŠ ")
            return
    
    # ä¿æŠ¤æªæ–½ï¼šåœ¨æ¸…ç©º sheet å‰ï¼Œæ£€æŸ¥ combined_df æ˜¯å¦åŒ…å«æ‰€æœ‰ç°æœ‰æ•°æ®
    if len(existing_data) > 1:
        original_row_count = len(existing_data) - 1  # å‡å»æ ‡é¢˜è¡Œ
        if len(combined_df) < original_row_count:
            print(f"âŒ é”™è¯¯ï¼šåˆå¹¶åæ•°æ®é‡å‡å°‘ï¼åŸå§‹ {original_row_count} è¡Œï¼Œåˆå¹¶å {len(combined_df)} è¡Œ")
            print(f"   åœæ­¢æ“ä½œï¼Œä¸æ¸…ç©º sheetï¼Œé¿å…æ•°æ®ä¸¢å¤±")
            raise ValueError(f"åˆå¹¶å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼š{original_row_count} è¡Œ -> {len(combined_df)} è¡Œ")
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
    if sort_by_date and 'Date' in combined_df.columns:
        try:
            # å°è¯•è§£ææ—¥æœŸï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            combined_df['Date_parsed'] = pd.to_datetime(
                combined_df['Date'], 
                errors='coerce',
                format='mixed'  # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
            )
            # å…ˆæŒ‰æ—¥æœŸæ’åºï¼Œç„¶ååˆ é™¤ä¸´æ—¶åˆ—
            combined_df = combined_df.sort_values('Date_parsed', ascending=True, na_position='last')
            combined_df = combined_df.drop('Date_parsed', axis=1)
            print(f"âœ… å·²æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰")
        except Exception as e:
            print(f"âš ï¸ æ—¥æœŸæ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")
            import traceback
            traceback.print_exc()
    
    # æ¸…ç©º sheet å¹¶é‡æ–°å†™å…¥ï¼ˆä¿ç•™æ ‡é¢˜è¡Œï¼‰
    worksheet.clear()
    if len(existing_data) > 0:
        worksheet.append_row(existing_data[0])  # å†™å…¥æ ‡é¢˜è¡Œ
    else:
        worksheet.append_row(combined_df.columns.tolist())
    
    # å†™å…¥æ•°æ®ï¼ˆåˆ†æ‰¹å†™å…¥ï¼‰
    if not combined_df.empty:
        batch_size = 100
        rows_written = 0
        for i in range(0, len(combined_df), batch_size):
            batch = combined_df.iloc[i:i+batch_size]
            values = batch.values.tolist()
            worksheet.append_rows(values)
            rows_written += len(batch)
        
        # ä¿æŠ¤æªæ–½ï¼šæ£€æŸ¥å†™å…¥çš„æ•°æ®é‡
        if rows_written != len(combined_df):
            print(f"âš ï¸ è­¦å‘Šï¼šå†™å…¥çš„æ•°æ®é‡ä¸åŒ¹é…ï¼æœŸæœ› {len(combined_df)} è¡Œï¼Œå®é™…å†™å…¥ {rows_written} è¡Œ")
        
        new_count = len(df)
        total_count = len(combined_df)
        print(f"âœ… å·²è¿½åŠ  {new_count} è¡Œæ–°æ•°æ®ï¼Œæ€»è®¡ {total_count} è¡Œï¼ˆå·²æŒ‰æ—¥æœŸæ’åºï¼‰åˆ° Google Sheets: {sheet_name}")

def _sort_sheet_by_date(worksheet, df: pd.DataFrame, headers: list):
    """
    å¯¹ sheet æŒ‰æ—¥æœŸæ’åºï¼ˆè¾…åŠ©å‡½æ•°ï¼‰
    """
    try:
        # æŒ‰æ—¥æœŸæ’åº
        if 'Date' in df.columns:
            df['Date_parsed'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.sort_values('Date_parsed', ascending=True, na_position='last')
            df = df.drop('Date_parsed', axis=1)
        
        # æ¸…ç©ºå¹¶é‡æ–°å†™å…¥
        worksheet.clear()
        worksheet.append_row(headers)
        
        batch_size = 100
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            values = batch.values.tolist()
            worksheet.append_rows(values)
    except Exception as e:
        print(f"âš ï¸ æ’åºå¤±è´¥: {e}")

def create_weekly_sheet(df: pd.DataFrame, spreadsheet_id: str, 
                        credentials_path: str = "google_credentials.json"):
    """
    åˆ›å»ºæ¯å‘¨çš„ sheetï¼ˆæŒ‰æ—¥æœŸå‘½åï¼‰
    """
    # è·å–æ—¥æœŸèŒƒå›´
    if 'Date' in df.columns:
        dates = pd.to_datetime(df['Date'], errors='coerce').dropna()
        if len(dates) > 0:
            start_date = dates.min().strftime("%Y-%m-%d")
            end_date = dates.max().strftime("%Y-%m-%d")
            sheet_name = f"Week {start_date} to {end_date}"
        else:
            sheet_name = f"Week {datetime.now().strftime('%Y-%m-%d')}"
    else:
        sheet_name = f"Week {datetime.now().strftime('%Y-%m-%d')}"
    
    export_to_sheets(df, spreadsheet_id, sheet_name, credentials_path)

def read_from_sheets(spreadsheet_id: str, sheet_name: str = None,
                    credentials_path: str = None) -> pd.DataFrame:
    """
    ä» Google Sheets è¯»å–æ•°æ®
    """
    client = get_sheets_client(credentials_path)
    spreadsheet = client.open_by_key(spreadsheet_id)
    
    if sheet_name:
        worksheet = spreadsheet.worksheet(sheet_name)
    else:
        worksheet = spreadsheet.sheet1
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    data = worksheet.get_all_values()
    
    if len(data) == 0:
        return pd.DataFrame()
    
    # ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
    df = pd.DataFrame(data[1:], columns=data[0])
    return df

